{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_FSM3_saliconDataset_v1.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fast Saliency Model (FSM3)\n",
        " Implementation of  the paper: **\"A compact deep architecture for real-time saliency prediction\"**.\n",
        "\n",
        "paper URL: https://doi.org/10.1016/j.image.2022.116671\n",
        "\n",
        "1- The \".ipynb\" is the implementation of the FSM3 model.\n",
        "\n",
        "2- The \".h\" file is the weights of the trained model.\n",
        "\n",
        "\n",
        "- The project codes were rewritten to be used in google Colab.\n",
        "- Copy the files in your google drive in \"Colab Notbooks\" folder. \n",
        "- To apply the model to your images, you should copy your images in the folder specified by \"input_imgs_path\".\n",
        "\n",
        "\n",
        "#Use following Reference:\n",
        "\n",
        "@article{zabihi2022compact,\n",
        "  title={A compact deep architecture for real-time saliency prediction},\n",
        "  author={Zabihi, Samad and Tavakoli, Hamed R and Borji, Ali and Mansoori, Eghbal},\n",
        "  journal={Signal Processing: Image Communication},\n",
        "  volume={104},\n",
        "  pages={116671},\n",
        "  year={2022},\n",
        "  publisher={Elsevier}\n",
        "}\n",
        "\n",
        "Resources: https://github.com/cyberstray/fast-saliency-model/\n"
      ],
      "metadata": {
        "id": "T43-UOqHHkwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_in_gdrive=input('Is there weight file in your google drive (y/n)?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdbKm1SHSXWu",
        "outputId": "b4010d85-11b0-49ff-86c1-707b8dd75960"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is there weight file in your google drive (y/n)?y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show Hardware Specification"
      ],
      "metadata": {
        "id": "oWfFWAwM2lr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat /proc/meminfo"
      ],
      "metadata": {
        "id": "4D4itUR02CCC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "QklFTZ5A2Ezd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Utilities\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BT8IqEy2_Sy5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlQf_kjYLKW6"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/examples.git\n",
        "import tensorflow as tf\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from __future__ import division\n",
        "import cv2\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.ndimage\n",
        "\n",
        "\n",
        "# input images\n",
        "shape_r = 224\n",
        "shape_c = 224\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVI4-OkAKO_k"
      },
      "source": [
        "def padding(img, shape_r=240, shape_c=320, channels=3):\n",
        "    img_padded = np.zeros((shape_r, shape_c, channels), dtype=np.uint8)\n",
        "    if channels == 1:\n",
        "        img_padded = np.zeros((shape_r, shape_c), dtype=np.uint8)\n",
        "    original_shape = img.shape\n",
        "    rows_rate = original_shape[0]/shape_r\n",
        "    cols_rate = original_shape[1]/shape_c\n",
        "    if rows_rate > cols_rate:\n",
        "        new_cols = (original_shape[1] * shape_r) // original_shape[0]\n",
        "        img = cv2.resize(img, (new_cols, shape_r))\n",
        "        if new_cols > shape_c:\n",
        "            new_cols = shape_c\n",
        "        img_padded[:, ((img_padded.shape[1] - new_cols) // 2):((img_padded.shape[1] - new_cols) // 2 + new_cols)] = img\n",
        "    else:\n",
        "        new_rows = (original_shape[0] * shape_c) // original_shape[1]\n",
        "        img = cv2.resize(img, (shape_c, new_rows))\n",
        "        if new_rows > shape_r:\n",
        "            new_rows = shape_r\n",
        "        img_padded[((img_padded.shape[0] - new_rows) // 2):((img_padded.shape[0] - new_rows) // 2 + new_rows), :] = img\n",
        "    return img_padded\n",
        "\n",
        "\n",
        "def preprocess_images(paths, shape_r, shape_c):\n",
        "    ims = np.zeros((len(paths), shape_r, shape_c, 3), dtype=np.float32)\n",
        "    for i, path in enumerate(paths):\n",
        "        #print(path)\n",
        "        original_image = cv2.imread(path)\n",
        "        #print(original_image.shape)\n",
        "        padded_image = padding(original_image, shape_r, shape_c, 3)\n",
        "        ims[i] = padded_image.astype(np.float32)/255.0\n",
        "    return ims\n",
        "\n",
        "\n",
        "def postprocess_predictions(pred, shape_r, shape_c):\n",
        "    predictions_shape = pred.shape\n",
        "    rows_rate = shape_r / predictions_shape[0]\n",
        "    cols_rate = shape_c / predictions_shape[1]\n",
        "    pred = pred / np.max(pred) * 255\n",
        "    if rows_rate > cols_rate:\n",
        "        new_cols = (predictions_shape[1] * shape_r) // predictions_shape[0]\n",
        "        pred = cv2.resize(pred, (new_cols, shape_r))\n",
        "        img = pred[:, ((pred.shape[1] - shape_c) // 2):((pred.shape[1] - shape_c) // 2 + shape_c)]\n",
        "    else:\n",
        "        new_rows = (predictions_shape[0] * shape_c) // predictions_shape[1]\n",
        "        pred = cv2.resize(pred, (shape_c, new_rows))\n",
        "        img = pred[((pred.shape[0] - shape_r) // 2):((pred.shape[0] - shape_r) // 2 + shape_r), :]\n",
        "    img = scipy.ndimage.filters.gaussian_filter(img, sigma=7)\n",
        "    img = img / np.max(img) * 255\n",
        "    return img\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Fast Saliency Model"
      ],
      "metadata": {
        "id": "9Ik6PE66FOss"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zixABwZ3kFIq"
      },
      "source": [
        "### create Encoder\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[shape_r, shape_c, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64 *2\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "    ]\n",
        "layers = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
        "model.trainable = True\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpxN6k94vkTF"
      },
      "source": [
        "import math\n",
        "from keras import backend as K\n",
        "\n",
        "def my_init(shape, dtype=None):\n",
        "    return K.random_normal(shape, dtype=dtype, seed=0.2)\n",
        "\n",
        "\n",
        "def kernel_init(shape, dtype=None):\n",
        "    np.random.seed(0)\n",
        "    kernel = np.random.normal(scale=0.5, size=shape).astype(float)\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def Conv2d_cd2(input, nout):\n",
        "    input_shape =input.get_shape().as_list()\n",
        "    out_channels = nout\n",
        "    _filter=kernel_init((3, 3, input_shape[-1], out_channels))\n",
        "    conv1 = tf.keras.layers.Conv2D(out_channels, 3, padding='same', activation='relu', use_bias=False,\n",
        "                                   kernel_initializer=kernel_init)\n",
        "    out_normal=conv1(input)\n",
        "    \n",
        "    if math.fabs(0.7 - 0.0) < 1e-8:\n",
        "        return out_normal \n",
        "    kernel_diff = tf.reduce_sum(input_tensor=_filter, axis=0, keepdims=True)\n",
        "    print('/kernel_diff.shape:', kernel_diff.get_shape())\n",
        "    kernel_diff = tf.reduce_sum(input_tensor=kernel_diff, axis=1, keepdims=True)\n",
        "    print('/kernel_diff.shape:', kernel_diff.get_shape())\n",
        "    kernel_diff = tf.tile(kernel_diff, [3, 3, 1, 1])\n",
        "    print('/kernel_diff.shape:', kernel_diff.get_shape())\n",
        "\n",
        "    out_diff = tf.nn.conv2d(input=input, filters=tf.dtypes.cast(kernel_diff, tf.float32), strides=[1, 1, 1, 1], padding='SAME')\n",
        "    return -(out_normal - 0.5 * out_diff)\n",
        "\n",
        "\n",
        "def CDC_BLOCK(x, nout):\n",
        "  xcd = Conv2d_cd2(x, nout)\n",
        "  xcd=tf.keras.layers.BatchNormalization(axis=-1)(xcd)\n",
        "  return xcd\n",
        "\n",
        "\n",
        "up_stack2 = [\n",
        "    pix2pix.upsample(16, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(16, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(16, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(16, 3),   # 32x32 -> 64x64\n",
        "]\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ-xhI_35QBm"
      },
      "source": [
        "def FSM3():\n",
        "  inputs = tf.keras.layers.Input(shape=[shape_r, shape_c, 3])\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = model(x)\n",
        "  n=1\n",
        "  x112=tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu')(skips[0])\n",
        "  x112 = tf.keras.layers.Concatenate()([x112, CDC_BLOCK(x112, n)])\n",
        "  x56=tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu')(skips[1])\n",
        "  x56 = tf.keras.layers.Concatenate()([x56, CDC_BLOCK(x56, n)])\n",
        "  x28=tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu')(skips[2])\n",
        "  x28 = tf.keras.layers.Concatenate()([x28, CDC_BLOCK(x28, n)])\n",
        "  x14=tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu')(skips[3])\n",
        "  x14 = tf.keras.layers.Concatenate()([x14, CDC_BLOCK(x14, n)])\n",
        "  x7=tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu')(skips[4])\n",
        "  x7 = tf.keras.layers.Concatenate()([x7, CDC_BLOCK(x7, n)])\n",
        "  x71=tf.keras.layers.Conv2D(1, 1, padding='same', activation='relu')(x7)\n",
        "  x71f=tf.keras.layers.Flatten()(x71)\n",
        "  fc_out7 = tf.keras.layers.Dense(56*56, activation='sigmoid', use_bias=False)(x71f)\n",
        "  fc2d = tf.keras.layers.Reshape((56, 56, 1)) (fc_out7)\n",
        "  fc2d=pix2pix.upsample(1, 3)(fc2d)\n",
        "  skips2=[x14, x28, x56, x112]\n",
        "  x=x7\n",
        "  \n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack2, skips2):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  #Central Difference Convolutional \n",
        "  # This is the last layers of the model\n",
        "  last1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')\n",
        "  last2 = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')\n",
        "  last3 = tf.keras.layers.Conv2DTranspose(1, 3, strides=2,\n",
        "      activation='relu', padding='same')  #64x64 -> 128x128 \n",
        "\n",
        "  x = tf.keras.layers.Concatenate()([x, fc2d])\n",
        "  x = last1(x)\n",
        "  x = last2(x)\n",
        "  x = last3(x)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=[x, x])\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m=FSM3()\n",
        "\n",
        "print(m.input_shape)\n",
        "print(m.output_shape)\n",
        "#m.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUNlXvuIw7Zw",
        "outputId": "885baeb0-43ca-4b4b-a39d-e21c9a50a0d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kernel_diff.shape: (1, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 1, 8, 1)\n",
            "/kernel_diff.shape: (3, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 1, 8, 1)\n",
            "/kernel_diff.shape: (3, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 1, 8, 1)\n",
            "/kernel_diff.shape: (3, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 1, 8, 1)\n",
            "/kernel_diff.shape: (3, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 3, 8, 1)\n",
            "/kernel_diff.shape: (1, 1, 8, 1)\n",
            "/kernel_diff.shape: (3, 3, 8, 1)\n",
            "(None, 224, 224, 3)\n",
            "[(None, 224, 224, 1), (None, 224, 224, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Section"
      ],
      "metadata": {
        "id": "nkYsaKcFCC6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download a sample image from URL and save in '/content/' folder\n",
        "!wget --output-document=lena.png https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3MmS3G2FVW8",
        "outputId": "15f7b104-20a9-4f0e-b92e-75ffa5c1d7b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-05 16:52:32--  https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 91.198.174.208, 2620:0:862:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|91.198.174.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 473831 (463K) [image/png]\n",
            "Saving to: ‘lena.png’\n",
            "\n",
            "\rlena.png              0%[                    ]       0  --.-KB/s               \rlena.png            100%[===================>] 462.73K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-08-05 16:52:32 (18.8 MB/s) - ‘lena.png’ saved [473831/473831]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if weight_in_gdrive=='y':\n",
        "  drive.mount('/content/gdrive')\n",
        "  m.trainable=True\n",
        "  m.load_weights('/content/gdrive/My Drive/Colab Notebooks/FSM3_weights_salicon2017.h5')\n",
        "m.trainable=False\n",
        "\n",
        "b_s=1 #batch size\n",
        "input_imgs_path='/content/'\n",
        "\n",
        "def generator_test(images, b_s):\n",
        "    images.sort()\n",
        "    counter = 0\n",
        "    while True:\n",
        "        ims=preprocess_images(images[counter:counter + b_s], shape_r, shape_c)\n",
        "        yield ims\n",
        "        counter = (counter + b_s) % len(images)\n",
        "\n",
        "images = [input_imgs_path + f for f in os.listdir(input_imgs_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "file_names = [f for f in os.listdir(input_imgs_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "images.sort()\n",
        "file_names.sort()\n",
        "\n",
        "predictions=[]\n",
        "print(\"Predicting for\", len(file_names), ' images in path: '+ input_imgs_path)\n",
        "predictions = m.predict_generator(generator_test(images, b_s), steps=len(file_names)/b_s)\n",
        "\n",
        "predictions=predictions[0]\n",
        "print('Done!')\n",
        "\n",
        "\n",
        "print('-- Writing output files with \"output\" prefix')\n",
        "for pred, name in zip(predictions, file_names):\n",
        "    original_image = cv2.imread(input_imgs_path + name, 0)\n",
        "    res = postprocess_predictions(pred, original_image.shape[0], original_image.shape[1])\n",
        "    cv2.imwrite(input_imgs_path+'output_' + '%s' % name[0:-4]+'.png', res.astype(int))\n",
        "print('Done! the output files are in folder: '+input_imgs_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qIrD0_c-8MP",
        "outputId": "992a44f7-f10d-46f7-e22c-20423ee350ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Predicting for 2  images in path: /content/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "-- Writing output files with \"output\" prefix\n",
            "Done! the output files are in folder: /content/\n"
          ]
        }
      ]
    }
  ]
}